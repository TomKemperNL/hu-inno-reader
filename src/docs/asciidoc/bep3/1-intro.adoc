== Gedistribueerde Systemen

=== Leerdoel

[quote]
“De student kan systemen, en niet slechts applicaties ontwikkelen, die in z'n geheel een echt probleem oplossen.” 


=== Inleiding

[quote, Leslie Lamport]
“A distributed system is one in which the failure of a computer you didn't even know existed can render your own computer unusable.” 

Tot nu toe heb je in je opleiding applicaties gemaakt die bepaalde problemen voor je stakeholders oplossen.
Dat is een nobel begin, maar vaak staan deze applicaties niet op zichzelf en wordt het tijd om na te denken over 
groepen applicaties die samen nog steeds de problemen van je stakeholders oplossen.

Zo'n groep applicaties die samen een doel dienen noemen we een gedistribueerd systeem. Een gedistribueerd systeem distribueert (verdeelt) de werking van het systeem over meerdere nodes (computers). 
Met een computer wordt in dit geval een 'rekenend ding' bedoeld, niet noodzakelijk het fysieke apparaat. Als jij twee applicaties op je laptop draait, dan kan men verwarrend genoeg spreken van twee computers (processen) op je computer (hardware-apparaat).
Deze gedistribueerde systemen hebben de verdiende reputatie om erg ingewikkeld te worden.

=== Voorbeelden

[quote, Tanenbaum & Van Steen (2017)]
"A distributed system is a collection of independent computers that appears to its users as a single coherent system."

==== Satelieten

[TODO:3 voorbeeld uit slides uitwerken]

==== Whatsapp

[TODO:3 voorbeeld uit slides uitwerken]

==== Bitcoin

[TODO:3 voorbeeld uit slides uitwerken]

=== Waarom distribueren?

Bovenstaande systemen zijn allemaal best intimiderend qua complexiteit. En tot nu toe werkte zo'n Spring @RestController toch prima? Waarom zou je jezelf dit aan willen doen?

In de literatuur (TODO:2 ref) hanteert men vaak de volgende standaardredenen:

* Fysieke redenen
* Performance / schaalbaarheid
* Reliability / fault-tolerance
* Integratie bestaande systemen

Allereerst Fysieke (of essentiele) redenen. Stel je neemt een systeem als Whatsapp, of een multi-player game als Counterstrike. Deze software systemen zijn per definitie gedistribueerd. Het zit in hun aard. Zonder het systeem uit meerdere processen (clients, servers) te laten bestaan kun je niet een bericht van een telefoon (client) via een server van Whatsapp naar andermans telefoon sturen. Evenzo kun je niet multiplayer iemand proberen neer te schieten als je niet alletwee achter je eigen computer zit (met evt. een centrale server ertussen).
Een ander voorbeeld is als er echt fysieke redenen zijn om een computer op een bepaalde plek te zetten, zoals bijv. bij de satelieten, of bij een MRI-scanner in een ziekenhuis. Die scanner gaat nergens heen, dus je zult de computer naar de scanner moeten brengen (en die wil z'n informatie natuurlijk delen met de rest van het ziekenhuis).

Twee computers kunnen meer rekenen dan één. Da's een waarheid als een koe. Er zijn zat problemen [TODO:3 voorbeelden, zoals eiwit vouwen, HADRON collider, maar ook wat moderners] die ook voor de grootste supercomputer te groot zijn. Dan kun je de last verdelen over meerdere computers, en heb je direct alle uitdagingen van een gedistribueerd systeem te pakken. Dichter bij huis kun je die performance-winst vaak ook wel vinden, want stel je wil bijv. je site op een Cloud-provider als AWS of Azure hosten, dan is het vaak goedkoper om twee kleinere machines te huren, dan om één grote te contracteren. 

Wat mij betreft de meest interessante, en veelal de meest praktische reden om een systeem te distribueren is om de reliability (betrouwbaarheid) te vergroten. Als je systeem uit slechts één proces bestaat, en dat proces functioneert niet (de server is *down*, het process is gecrashed, het netwerk laat je er niet bij), dan doet niets in je systeem het. Als je je systeem uit meerdere onderdelen laat bestaan, dan kan je systeem 'een beetje stuk' zijn.

Het klinkt raar om 'een beetje stuk' als iets goeds te zien, maar veelal zijn er oorzaken buiten je controle die er voor zorgen dat het maar wat handig is dat bepaalde delen van je systeem gewoon doordraaien, als er andere delen even uit liggen. Denk hierbij bijv. aan klassieke hardware-fouten (hardware gaat stuk, iemand trekt de stekker eruit), rampen (brand in het datacenter), of onschuldiger, een software update (dan moet het proces ook eventjes uit).

****
Persoonlijk was reliability voor mij een groot goed. Vroeger was ik verantwoordelijk voor een grote applicatie die uit 1 proces bestond, maar wel uit veel verschillende delen. Updates aan deze applicatie uitvoeren was erg lastig, want dan was alle functionaliteit van ons bedrijf onbereikbaar.

Het gevolg was dat we 3-4 keer per jaar, midden in de nacht, een mega-release deden. Dat ging eigenlijk nooit goed en was altijd enorm stressen om alles voor 06:00 's ochtends weer 'goed genoeg' te krijgen. 

Een ander gevolg was dat als 1 deel van de applicatie een foutje had (en bijv. de CPU van de server op 100% zette), dat de hele applicatie het niet meer deed.

Deze twee zaken zorgden voor een bedrijfssfeer en cultuur waarbij er altijd stress en druk was, en vooral _schuld_ een grote rol speelde. Op het ergste moment zette een collega van mij alle instructies die hij van projectmanagers ontving in een git-repository, zodat hij achteraf kon bewijzen dat hij een bepaalde instructie echt ontvangen had (omdat er bij grote outages vaak geclaimed werd dat "De developers zomaar wat deden, waardoor het allemaal mis ging").

Door de technieken van deze cursus te leren en toe te passen was ons bedrijf in ong. een jaar tijd vervormt van een hele nare plek om te werken, naar een hele prima plek. Door de applicatie in stukjes te hakken, high-availability patterns toe te passen, en vooral messaging te gebruiken konden we gewoon overdag onder werktijd deployen. En omdat het zo makkelijk was gingen we dat ook veel vaker doen (meestal elke twee weken, maar ook vaak zat vaker). En als er problemen op productie waren, dan konden we vaak dat stuk van de aplicatie gericht even 'offline voor maintenance' halen, en kon de rest doordraaien. Niet ideaal, maar _stukken_ beter dan het alternatief.

De stof van dit semester heeft me echt rust, en een veel fijnere baan gegeven.

-Tom
****

Tot slot hebben we nog de aanwezigheid van bestaande systemen die ons kunnen dwingen om de uitdagingen van een gedistribueerd systeem het hoofd te bieden. Op school heb je er niet zoveel mee te maken, maar in de meeste praktijk situaties heb je te maken met bestaande software systemen. De studentenlijst moet uit Osiris komen, en correct in Canvas geladen worden. Cijfers uit Testvision moeten teruggezet worden bij een assignment in Canvas, etc. (dit is allemaal wensdenken overigens, de HU is niet zo geautomatiseerd).

=== Waarom niet?

[quote, Fowler (2003)]
"First Law of Distributed Object Design: Don't distribute your objects!"

Genoeg redenen om een systeem te distribueren dus. Helaas heeft het ook zeker nadelen. Een klassieke opsomming zijn de 8 fallaciesfootnote:[Het woord _fallacie_ is geen nederlands woord, we verbasteren het een beetje. Net zoals we van een _class_ kunnen _inheriten_.] (drogredenen) van gedistribueerde systemen [TODO:2 Ref Deutsch & Gossling, deze is lastig te vinden?]:

* The network is reliable
* Latency is zero
* Bandwidth is infinite
* The network is secure
* The topology does not change
* There is one administrator
* Transport cost is zero
* The network is homogenous

Al deze fallacies zijn makkelijk te negeren als developer. We zitten immers 90% van onze tijd lekker te klungelen op ```http://localhost:8080```. Als we communiceren met localhost (ook wel bekend als ip-adres 127.0.0.1) dan is het netwerk _wel_ reliable. Het is zeer onwaarschijnlijk dat je zogeheten loopback-adapter het niet doet.

De latency, plat gezegd de vertraging tussen het versturen van pakketjes (_packets_) informatie en het aankomen er van, is via die loopback-adapter ook nagenoeg nul. En de bandbreedte, dus hoe groot de packets informatie mogen zijn en hoeveel data er in totaal verstuurd kan worden is praktisch oneindig. Kortom, op localhost hebben we met deze fallacies niet zoveel te maken.

Het is daarom vaak een nare verrassing als je daar ineens _wel_ rekening mee moet houden. De eerste drie fallacies spreken voor zich, als je lange kabels hebt kan er van alles mee gebeuren, er kunnen zelfs haaien in bijten [TODO:3 ref haaien]. En als het niet jouw netwerk is, dan kun je er ook kosten voor moeten betalen (Cloud providers zoals Azure en AWS kunnen je hier vaak met een onverwacht gepeperde rekening presenteren).

Als je niet mag aannemen dat het netwerk veilig (_secure_) is, dan betekent dat dat je allerlei extra infrastructuur zult moeten hanteren om te zorgen dat je toch veilig tussen verschillende computers/applicaties/processen kan communiceren. TLS (Transport Layer Security) is een veelgebruikte oplossing hiervoor. Dit ken je waarschijnlijk zelf als het verschil tussen http en http&*S*.

De aanname dat er één administrator is (vaak gekoppeld met de aanname dat jij dat bent, aangezien jij immers de baas bent van je eigen localhost-omgeving) zorgt vaak voor onverwachte problemen. Het kan bijv. een stuk langer duren voordat je überhaupt toegang krijgt tot je productie-database, of je kan er ineens niet meer bij omdat je een mailtje over het hoofd hebt gezien. Anderzijd s kan het ook zo zijn dat een ander systeem dat jij nodig hebt (bijv. een gedeeld inlog-systeem) besluit een update uit te voeren. Dan heb je ineens twee problemen: tijdens de update heb je een overduidelijk probleem, want het andere systeem ligt eruit, maar je moet ook uitzoeken of je na de update nog wel correct met het systeem kan praten!

Tot slot hebben we nog twee stukjes yargon. De aanname dat de topologie niet verandert betekent dat de _abstracte vorm_ van het netwerk niet verandert. Stel je hebt een kantoornetwerk, waarbij elke computer verbonden is met een switch, en die switch gaat via een router naar buiten. Als we dan al die computers een beetje gaan verschuiven verandert het netwerk dan welliswaar van vorm (alle computers staan op een andere plek), maar niet van abstracte vorm (het zijn nog steeds een ster van computers aan een switch, die vanuit daar naar een router gaan). In internet-systemen kom je dit vaak tegen als lange-afstand-routes door bijv. BGP [TODO:2 ref BGP] veranderen. 

De laatste is de aanname dat het netwerk _homogeen_ is, oftewel dat het uit dezelfde soort apparaten bestaat. Tussen jouw systemen staan vaak allerlei andere apparaten, en je bent vaak een beetje afhankelijk van wat voor soort protocollen, dataformaten en groottes deze apparaten ondersteunen. Een voorbeeld hiervan is de enorme vertraging die de overstap van IPv4 naar IPv6 op is gelopen [TODO:3 IPv4-IPv6 link]. Dichter bij huis was (is?) er een reverse-proxy ergens voor Canvas die Cookie-headers afkapt, sommige studenten (die bijv. bij veel verschillende *.hu.nl sites waren ingelogd) konden soms (want het verkeer ging niet altijd over die server) niet inloggen bij Canvas.

=== Voorbeelden

We hebben eerder gekeken naar grote gedistribueerde systemen. Maar als we goed kijken zien we deze problemen (en kansen) al in veel kleinere systemen terug.

==== Databases

==== Doodnormale webapp

=== Distribution Transparancy

Waarom denken we bij dat soort kleinere situaties vaak niet aan gedistribueerde systemen? Dat komt omdat hun gedistribueerde aard goed verstopt is! Als je online aan het shoppen bent, dan voelt de frontend echt als onderdeel van de winkel (terwijl het toch echt op jouw pc draait), en je denkt niet aan hun database, of webserver, of loadbalancer, of inlogsysteem: er is gewoon _de winkel_.

Dit principe, dat je niet doorhebt dat er eigenlijk vele verschillende processen een rol spelen noemen we met een chique woord _Distribution Transparancy_ cite:[tanenbaum_distributed_2017]. Deze term is een beetje verwarrend, want Distribution Transparancy is behaald als men _niet_ kan zien dat het systeem gedistribueerd isfootnote:[Ik vind dit verwarrend, want ik zou zeggen dat als de distributie transparant is, dat je dan _juist_ goed kan zien hoe de verschillende onderdelen in elkaar zitten. Maar dit is dus *niet* hoe deze term in de praktijk gebruikt wordt. -Tom].

Uiteraard is deze transparantie nooit volmaakt, en kan die op verschillende wijzes complexiteit verbergen, of juist laten doorschemeren.

* Access Transparancy:
  Hiermee bedoelen we dat het niet precies duidelijk is hoe we bij bepaalde informatie komen. Krijgen we de informatie direct? Of zit er een tussenpersoon tussen? Als jij naar een website gaat zit daar vaak een https://www.x.y adres voor, maar het zou maar zo kunnen dat de ene helft van de website van een web-winkel-server komt, terwijl bijv. de nieuwspagina van een CMS-server komt, en de bedrijfsinformatie op een derde plek gehost wordt. Door een slimme Gateway/API-facade/etc. er voor te zetten merk je dit niet. Totdat ineens de ene helft van de site offline is, en de andere niet. Dan wordt duidelijk dat er altijd al meerdere delen waren.
* Location Transparancy:
  Er zijn vele truken om te voorkomen dat men exact weet _waar_ een bepaalde service gehost wordt. Neem bijv. urls. Als we bijv. kijken waar https://utrecht.nl gehost wordt, dan is het een redelijke aanname dat dit in Utrecht is, en inderdaad, dat blijkt (op moment van schrijven tenminste) te kloppenfootnote:[Het commando 'ping -4 utrecht.nl' geeft je een ip adres waar je de locatie van kan opzoeken]. Dan zou het ook logisch zijn om aan te nemen dat https://hu.nl ook in Utrecht te vinden is. Maar nee, de Hogeschool Utrecht site woont in een datacenter in Amsterdam. De exacte locatie is dus niet te zien, een vorm van transparantie.
* Replication Transparancy:
  Grote websites krijgen vaak zoveel bezoekers dat één server het niet allemaal aan kan. Performance was immers een reden om een gedistribueerd systeem te bouwen. Desalniettemin kun je als het goed is niet zien dat er meerdere servers gebruikt worden. Wie weet hoeveel servers er achter https://hu.nl schuilen? Een genantfootnote:[Uiteraard heb ik exact dit soort zaken met schaamte in productie gedraaid... Het was opvallend hoe ontzettend weinig dit uitmaakt voor veel non-technische mensen. Ik durfde me echter niet meer op Developer-meetups te vertonen! -Tom] alternatief zou bijv. zijn als er op drukke Open Dagen sommige opleidingen zouden draaien op https://opendag1.hu.nl/ICT en anderen op https://opendag2.hu.nl/tandheelkunde.
* Concurrency Transparancy:
  Als ik op een grote internet webwinkel zit te browsen achter mijn computer, dan _voelt_ het alsof ik de enige klant in de winkel ben. Ik zie in elk geval geen andere klanten! Dus het lijkt alsof die server alleen met mij bezig is, wat natuurlijk een enorme eer is. In werkelijkheid is die server met tichduizend mensen tegelijk bezig. Allemaal onzichtbaar (transparant) voor mij.
* Failure Transparancy:
  Je hebt vast wel eens meegemaakt dat je ineens, in plaats van een mooie pagina, geconfronteerd wordt met een kale HTML pagina, waarop ineens staat dat er geen connectie gemaakt kan worden met database XYZ. Meestal ook nog met een mooie stacktrace, en een hint wat voor server/framework/database gebruikt wordt. Vervolgens haal je je schouders op, en druk je op F5 om te refreshen, en voilá alles werkt weer. In dat geval ben je even mooi met je neus op de feiten gedrukt dat deze site een aparte database gebruikte, dat er een verschil tussen backend-en-frontend framework is, en meer van zulks.

=== Integratiestijlen

Grofweg zijn er 4 stijlen te vinden waarop we applicaties met elkaar integreren cite:[hohpe_enterprise_2012]

* File Transfer
* Shared Database
* RPC
* Messaging

eeeh, wat is hier aan de hand?

