== Gedistribueerde Systemen

=== Wat is het?

[quote, Leslie Lamport]
“A distributed system is one in which the failure of a computer you didn't even know existed can render your own computer unusable.” 

Tot nu toe heb je in je opleiding applicaties gemaakt die bepaalde problemen voor je stakeholders oplossen.
Dat is een nobel begin, maar vaak staan deze applicaties niet op zichzelf en wordt het tijd om na te denken over 
groepen applicaties die samen nog steeds de problemen van je stakeholders oplossen.

Zo'n groep applicaties die samen een doel dienen noemen we een gedistribueerd systeem. Een gedistribueerd systeem distribueert (verdeelt) de werking van het systeem over meerdere nodes (computers). 
Met een computer wordt in dit geval een 'rekenend ding' bedoeld, niet noodzakelijk het fysieke apparaat. Als jij twee applicaties op je laptop draait, dan kan men verwarrend genoeg spreken van twee computers (processen) op je computer (hardware-apparaat).
Deze gedistribueerde systemen hebben de verdiende reputatie om erg ingewikkeld te worden.

=== Voorbeelden

[quote, Tanenbaum & Van Steen (2017)]
"A distributed system is a collection of independent computers that appears to its users as a single coherent system."

==== Satelieten

[TODO:3 voorbeeld uit slides uitwerken]

==== Whatsapp

[TODO:3 voorbeeld uit slides uitwerken]

==== Bitcoin

[TODO:3 voorbeeld uit slides uitwerken]

=== Waarom distribueren?

Bovenstaande systemen zijn allemaal best intimiderend qua complexiteit. En tot nu toe werkte zo'n Spring @RestController toch prima? Waarom zou je jezelf dit aan willen doen?

In de literatuur (TODO:2 ref) hanteert men vaak de volgende standaardredenen:

* Fysieke redenen
* Performance / schaalbaarheid
* Reliability / fault-tolerance
* Integratie bestaande systemen

Allereerst Fysieke (of essentiele) redenen. Stel je neemt een systeem als Whatsapp, of een multi-player game als Counterstrike. Deze software systemen zijn per definitie gedistribueerd. Het zit in hun aard. Zonder het systeem uit meerdere processen (clients, servers) te laten bestaan kun je niet een bericht van een telefoon (client) via een server van Whatsapp naar andermans telefoon sturen. Evenzo kun je niet multiplayer iemand proberen neer te schieten als je niet alletwee achter je eigen computer zit (met evt. een centrale server ertussen).
Een ander voorbeeld is als er echt fysieke redenen zijn om een computer op een bepaalde plek te zetten, zoals bijv. bij de satelieten, of bij een MRI-scanner in een ziekenhuis. Die scanner gaat nergens heen, dus je zult de computer naar de scanner moeten brengen (en die wil z'n informatie natuurlijk delen met de rest van het ziekenhuis).

Twee computers kunnen meer rekenen dan één. Da's een waarheid als een koe. Er zijn zat problemen [TODO:3 voorbeelden, zoals eiwit vouwen, HADRON collider, maar ook wat moderners] die ook voor de grootste supercomputer te groot zijn. Dan kun je de last verdelen over meerdere computers, en heb je direct alle uitdagingen van een gedistribueerd systeem te pakken. Dichter bij huis kun je die performance-winst vaak ook wel vinden, want stel je wil bijv. je site op een Cloud-provider als AWS of Azure hosten, dan is het vaak goedkoper om twee kleinere machines te huren, dan om één grote te contracteren. 

Wat mij betreft de meest interessante, en veelal de meest praktische reden om een systeem te distribueren is om de reliability (betrouwbaarheid) te vergroten. Als je systeem uit slechts één proces bestaat, en dat proces functioneert niet (de server is *down*, het process is gecrashed, het netwerk laat je er niet bij), dan doet niets in je systeem het. Als je je systeem uit meerdere onderdelen laat bestaan, dan kan je systeem 'een beetje stuk' zijn.

Het klinkt raar om 'een beetje stuk' als iets goeds te zien, maar veelal zijn er oorzaken buiten je controle die er voor zorgen dat het maar wat handig is dat bepaalde delen van je systeem gewoon doordraaien, als er andere delen even uit liggen. Denk hierbij bijv. aan klassieke hardware-fouten (hardware gaat stuk, iemand trekt de stekker eruit), rampen (brand in het datacenter), of onschuldiger, een software update (dan moet het proces ook eventjes uit).

****
Persoonlijk was reliability voor mij een groot goed. Vroeger was ik verantwoordelijk voor een grote applicatie die uit 1 proces bestond, maar wel uit veel verschillende delen. Updates aan deze applicatie uitvoeren was erg lastig, want dan was alle functionaliteit van ons bedrijf onbereikbaar.

Het gevolg was dat we 3-4 keer per jaar, midden in de nacht, een mega-release deden. Dat ging eigenlijk nooit goed en was altijd enorm stressen om alles voor 06:00 's ochtends weer 'goed genoeg' te krijgen. 

Een ander gevolg was dat als 1 deel van de applicatie een foutje had (en bijv. de CPU van de server op 100% zette), dat de hele applicatie het niet meer deed.

Deze twee zaken zorgden voor een bedrijfssfeer en cultuur waarbij er altijd stress en druk was, en vooral _schuld_ een grote rol speelde. Op het ergste moment zette een collega van mij alle instructies die hij van projectmanagers ontving in een git-repository, zodat hij achteraf kon bewijzen dat hij een bepaalde instructie echt ontvangen had (omdat er bij grote outages vaak geclaimed werd dat "De developers zomaar wat deden, waardoor het allemaal mis ging").

Door de technieken van deze cursus te leren en toe te passen was ons bedrijf in ong. een jaar tijd vervormt van een hele nare plek om te werken, naar een hele prima plek. Door de applicatie in stukjes te hakken, high-availability patterns toe te passen, en vooral messaging te gebruiken konden we gewoon overdag onder werktijd deployen. En omdat het zo makkelijk was gingen we dat ook veel vaker doen (meestal elke twee weken, maar ook vaak zat vaker). En als er problemen op productie waren, dan konden we vaak dat stuk van de aplicatie gericht even 'offline voor maintenance' halen, en kon de rest doordraaien. Niet ideaal, maar _stukken_ beter dan het alternatief.

De stof van dit semester heeft me echt rust, en een veel fijnere baan gegeven.

-Tom
****

Tot slot hebben we nog de aanwezigheid van bestaande systemen die ons kunnen dwingen om de uitdagingen van een gedistribueerd systeem het hoofd te bieden. Op school heb je er niet zoveel mee te maken, maar in de meeste praktijk situaties heb je te maken met bestaande software systemen. De studentenlijst moet uit Osiris komen, en correct in Canvas geladen worden. Cijfers uit Testvision moeten teruggezet worden bij een assignment in Canvas, etc. (dit is allemaal wensdenken overigens, de HU is niet zo geautomatiseerd). Deze systemen zijn allemaal afzonderlijk van elkaar ingekocht, en vaak in compleet andere programmeertalen geschreven, dus in deze gevallen kun je er niet één applicatie van maken, ook al zou je het willen.

[#fallacies]
=== Waarom niet?

Genoeg redenen om een systeem te distribueren dus. Helaas heeft het ook zeker nadelen. Een klassieke opsomming zijn de 8 fallaciesfootnote:[Het woord _fallacie_ is geen nederlands woord, we verbasteren het een beetje. Net zoals we van een _class_ kunnen _inheriten_.] (drogredenen) van gedistribueerde systemen [TODO:2 Ref Deutsch & Gossling, deze is lastig te vinden?]:

* The network is reliable
* Latency is zero
* Bandwidth is infinite
* The network is secure
* The topology does not change
* There is one administrator
* Transport cost is zero
* The network is homogenous

Al deze fallacies zijn makkelijk te negeren als developer. We zitten immers 90% van onze tijd lekker te klungelen op ```http://localhost:8080```. Als we communiceren met localhost (ook wel bekend als ip-adres 127.0.0.1) dan is het netwerk _wel_ reliable. Het is zeer onwaarschijnlijk dat je zogeheten loopback-adapter het niet doet.

De latency, plat gezegd de vertraging tussen het versturen van pakketjes (_packets_) informatie en het aankomen er van, is via die loopback-adapter ook nagenoeg nul. En de bandbreedte, dus hoe groot de packets informatie mogen zijn en hoeveel data er in totaal verstuurd kan worden is praktisch oneindig. Kortom, op localhost hebben we met deze fallacies niet zoveel te maken.

Het is daarom vaak een nare verrassing als je daar ineens _wel_ rekening mee moet houden. De eerste drie fallacies spreken voor zich, als je lange kabels hebt kan er van alles mee gebeuren, er kunnen zelfs haaien in bijten [TODO:3 ref haaien]. En als het niet jouw netwerk is, dan kun je er ook kosten voor moeten betalen (Cloud providers zoals Azure en AWS kunnen je hier vaak met een onverwacht gepeperde rekening presenteren).

Als je niet mag aannemen dat het netwerk veilig (_secure_) is, dan betekent dat dat je allerlei extra infrastructuur zult moeten hanteren om te zorgen dat je toch veilig tussen verschillende computers/applicaties/processen kan communiceren. TLS (Transport Layer Security) is een veelgebruikte oplossing hiervoor. Dit ken je waarschijnlijk zelf als het verschil tussen http en http&*S*.

De aanname dat er één administrator is (vaak gekoppeld met de aanname dat jij dat bent, aangezien jij immers de baas bent van je eigen localhost-omgeving) zorgt vaak voor onverwachte problemen. Het kan bijv. een stuk langer duren voordat je überhaupt toegang krijgt tot je productie-database, of je kan er ineens niet meer bij omdat je een mailtje over het hoofd hebt gezien. Anderzijd s kan het ook zo zijn dat een ander systeem dat jij nodig hebt (bijv. een gedeeld inlog-systeem) besluit een update uit te voeren. Dan heb je ineens twee problemen: tijdens de update heb je een overduidelijk probleem, want het andere systeem ligt eruit, maar je moet ook uitzoeken of je na de update nog wel correct met het systeem kan praten!

Tot slot hebben we nog twee stukjes yargon. De aanname dat de topologie niet verandert betekent dat de _abstracte vorm_ van het netwerk niet verandert. Stel je hebt een kantoornetwerk, waarbij elke computer verbonden is met een switch, en die switch gaat via een router naar buiten. Als we dan al die computers een beetje gaan verschuiven verandert het netwerk dan welliswaar van vorm (alle computers staan op een andere plek), maar niet van abstracte vorm (het zijn nog steeds een ster van computers aan een switch, die vanuit daar naar een router gaan). In internet-systemen kom je dit vaak tegen als lange-afstand-routes door bijv. BGP [TODO:2 ref BGP] veranderen. 

De laatste is de aanname dat het netwerk _homogeen_ is, oftewel dat het uit dezelfde soort apparaten bestaat. Tussen jouw systemen staan vaak allerlei andere apparaten, en je bent vaak een beetje afhankelijk van wat voor soort protocollen, dataformaten en groottes deze apparaten ondersteunen. Een voorbeeld hiervan is de enorme vertraging die de overstap van IPv4 naar IPv6 op is gelopen [TODO:3 IPv4-IPv6 link]. Dichter bij huis was (is?) er een reverse-proxy ergens voor Canvas die Cookie-headers afkapt, sommige studenten (die bijv. bij veel verschillende *.hu.nl sites waren ingelogd) konden soms (want het verkeer ging niet altijd over die server) niet inloggen bij Canvas.

=== Voorbeelden

We hebben eerder gekeken naar grote gedistribueerde systemen. Maar als we goed kijken zien we deze problemen (en kansen) al in veel kleinere systemen terug.

==== Databases

Op het moment dat we een externe database gebruiken voor onze webservice hebben we eigenlijk al een gedistribueerd systeem!

Meestal is de verbinding tussen die twee systemen zo goed dat we er niet over na hoeven denken. Maar soms gaat die vlieger niet op. Bij grote hoeveelheden data, of bij grote hoeveelheden requests kun je toch merken dat er vertraging ontstaat bij de interactie met de database.

Denk bijv. terug aan het _N+1_-probleem in een ORM. Dat is het probleem waarbij je entity hebt, met een referentie naar een collectie van andere entities, die op hun beurt een _lazy_ referentie hebben naar een derde entity. Een concreet voorbeeld is bijv. een _Klas_ met _Student_-objecten, en elk _Student_ object heeft een lazy referentie naar diens _Adres_. Als je zonder goed na te denken (_eager fetching_) een lijstje van adressen van een klas wilt teruggeven in een controller, dan zal jouw ORM framework per student een losse call naar de database doen om dat ene adres op te halen. Dat is op localhost al niet fijn, maar als je database enkele tientallen miliseconden verder weg staat ga je dat al heel snel merken. Latency is immers niet zero.

Een ander geval waarin je gedistribueerde systemen problematiek tegenkomt is wanneer je _caching_ gebruikt. Stel je hebt een dure query die een paar seconden duurt, maar de informatie verandert niet heel vaak... Dan is het vaak slim om de resultaten een paar seconden of minuten in het geheugen te houden, en andere requests voor dezelfde informatie direct uit het geheugen te beantwoorden. Op dat moment zijn er twee kopieën van de data aanwezig in het systeem (de waarheid in de database, en jouw lokale gecachede kopie). Zorgen dat die twee niet te ver uit elkaar lopen kan snel vrij tricky worden, en de bandwith is niet infinite.

==== Doodnormale webapp

Een doorsnee webapp zien we vaak niet als gedistribueerd systeem, maar de combinatie frontend en backend is er toch echt één. Meestal stuur je als frontend een pakketje html, javascript en css naar de client-computer, en die draait op diens systeem de geleverde code.

Meestal hoef je daar niet te diep over na te denken, maar in sommige gevallen wordt dat ineens erg belangrijk. Stel je hebt bijv. gegevens en plaatjes in een beveiligde Cloud-opslag account staan... De meeste cloud-providers hebben een javascript API zodat je in javascript bij deze gegevens zou kunnen, maar dan is dat nog steeds geen goed idee! Want je zou in dit geval dan jouw inlog-gegevens moeten meesturen met de frontend-bundle, en hoe goet je dat ook probeert te verstoppen of te verbergen, die informatie gaat te achterhalen zijn... De network is immers niet secure.

=== Distribution Transparancy

Waarom denken we bij dat soort kleinere situaties vaak niet aan gedistribueerde systemen? Dat komt omdat hun gedistribueerde aard goed verstopt is! Als je online aan het shoppen bent, dan voelt de frontend echt als onderdeel van de winkel (terwijl het toch echt op jouw pc draait), en je denkt niet aan hun database, of webserver, of loadbalancer, of inlogsysteem: er is gewoon _de winkel_.

Dit principe, dat je niet doorhebt dat er eigenlijk vele verschillende processen een rol spelen noemen we met een chique woord _Distribution Transparancy_ cite:[tanenbaum_distributed_2017]. Deze term is een beetje verwarrend, want Distribution Transparancy is behaald als men _niet_ kan zien dat het systeem gedistribueerd isfootnote:[Ik vind dit verwarrend, want ik zou zeggen dat als de distributie transparant is, dat je dan _juist_ goed kan zien hoe de verschillende onderdelen in elkaar zitten. Maar dit is dus *niet* hoe deze term in de praktijk gebruikt wordt. -Tom].

Uiteraard is deze transparantie nooit volmaakt, en kan die op verschillende wijzes complexiteit verbergen, of juist laten doorschemeren.

* Access Transparancy:
  Hiermee bedoelen we dat het niet precies duidelijk is hoe we bij bepaalde informatie komen. Krijgen we de informatie direct? Of zit er een tussenpersoon tussen? Als jij naar een website gaat zit daar vaak een https://www.x.y adres voor, maar het zou maar zo kunnen dat de ene helft van de website van een web-winkel-server komt, terwijl bijv. de nieuwspagina van een CMS-server komt, en de bedrijfsinformatie op een derde plek gehost wordt. Door een slimme Gateway/API-facade/etc. er voor te zetten merk je dit niet. Totdat ineens de ene helft van de site offline is, en de andere niet. Dan wordt duidelijk dat er altijd al meerdere delen waren.
* Location Transparancy:
  Er zijn vele truken om te voorkomen dat men exact weet _waar_ een bepaalde service gehost wordt. Neem bijv. urls. Als we bijv. kijken waar https://utrecht.nl gehost wordt, dan is het een redelijke aanname dat dit in Utrecht is, en inderdaad, dat blijkt (op moment van schrijven tenminste) te kloppenfootnote:[Het commando 'ping -4 utrecht.nl' geeft je een ip adres waar je de locatie van kan opzoeken]. Dan zou het ook logisch zijn om aan te nemen dat https://hu.nl ook in Utrecht te vinden is. Maar nee, de Hogeschool Utrecht site woont in een datacenter in Amsterdam. De exacte locatie is dus niet te zien, een vorm van transparantie.
* Replication Transparancy:
  Grote websites krijgen vaak zoveel bezoekers dat één server het niet allemaal aan kan. Performance was immers een reden om een gedistribueerd systeem te bouwen. Desalniettemin kun je als het goed is niet zien dat er meerdere servers gebruikt worden. Wie weet hoeveel servers er achter https://hu.nl schuilen? Een genantfootnote:[Uiteraard heb ik exact dit soort zaken met schaamte in productie gedraaid... Het was opvallend hoe ontzettend weinig dit uitmaakt voor veel non-technische mensen. Ik durfde me echter niet meer op Developer-meetups te vertonen! -Tom] alternatief zou bijv. zijn als er op drukke Open Dagen sommige opleidingen zouden draaien op https://opendag1.hu.nl/ICT en anderen op https://opendag2.hu.nl/tandheelkunde.
* Concurrency Transparancy:
  Als ik op een grote internet webwinkel zit te browsen achter mijn computer, dan _voelt_ het alsof ik de enige klant in de winkel ben. Ik zie in elk geval geen andere klanten! Dus het lijkt alsof die server alleen met mij bezig is, wat natuurlijk een enorme eer is. In werkelijkheid is die server met tichduizend mensen tegelijk bezig. Allemaal onzichtbaar (transparant) voor mij.
* Failure Transparancy:
  Je hebt vast wel eens meegemaakt dat je ineens, in plaats van een mooie pagina, geconfronteerd wordt met een kale HTML pagina, waarop ineens staat dat er geen connectie gemaakt kan worden met database XYZ. Meestal ook nog met een mooie stacktrace, en een hint wat voor server/framework/database gebruikt wordt. Vervolgens haal je je schouders op, en druk je op F5 om te refreshen, en voilá alles werkt weer. In dat geval ben je even mooi met je neus op de feiten gedrukt dat deze site een aparte database gebruikte, dat er een verschil tussen backend-en-frontend framework is, en meer van zulks.

=== Integratiestijlen

Grofweg zijn er 4 stijlen te vinden waarop we applicaties met elkaar integreren cite:[hohpe_enterprise_2012]

* File Transfer
* Shared Database
* RPC
* Messaging

File transfer gaat over het exporteren van grote hoeveelheden data (dumps) uit systeem A, om die vervolgens in systeem B in te laden. Dat kan geautomatiseerd, of met de hand (download, en vervolgens upload). Shared Database is de voor de hand liggende strategie om meerdere processen direct op dezelfde database aan te sluiten, dan is de integratie ook snel geregeld.

File transfers zijn vaak nogal log, duren lang, en zijn nogal fragiel. Als er eeeergens iets mis gaat moet je vaak het hele proces annuleren, omdat je niet met zekerheid kan zeggen dat de rest van de data nog wel veilig geïmporteerd is.

Een Shared Database klinkt als een goed idee, en voor hele kleine integraties kan het ook nog wel. In de praktijk leidt deze aanpak echter al snel tot frictie. We zijn gewend om onze database te kunnen updaten (kolommetje erbij, kolommetje renamen, etc.). Dit gaat enorm fout als er meerdere applicaties op dezelfde database draaien. Het is al lastig om dit allemaal goed te onthouden als je zelf als solo-developer al die applicaties onderhoudt, maar als de applicaties ook nog eens door verschillende teams van developers onderhouden worden, dan wordt deze aanpak al snel één grote chaos.

**** 
Jarenlang had één van onze klanten een systeem waarbij bepaalde rapporten die bij hen op een of ander dashboard getoond werden rechtstreeks op onze database gedraaid werden. Deze afspraak was lang voor mijn tijd gemaakt en kon, voor zover men zei, niet worden aangepast.

Dit zorgde er voor dat grote delen van ons datamodel compleet vast zaten en niet verbeterd konden worden. Een groot probleem omdat we een multi-tenant (meerdere-klanten-op-1-systeem) applicatie hadden.

Uiteindelijk werd het zo erg dat we maar de hele codebase geforked hebben zodat al onze andere klanten geen last hadden van deze deal. Jarenlang hebben we dus bij elke wijziging rekening moeten houden met 'kan het ook voor die ene klant?!'.

Kortom, pas op met Shared Databases. Je hebt supersnel een eerste versie, en daarna vaak jarenlang ellende.

-Tom
****

Een Remote Procedure Call (RPC) is een bericht dat een methode aanroept in een extern systeem. Dat is een hele gangbare en flexibele manier van integreren waar we in <<_remote_procedure_calls>> dieper op in zullen gaan.

Tot slot kun je gebruik maken van asynchrone messaging systemen. Dat is een zeer flexibele, maar ook in eerste instantie lastige, manier van integreren. Hierbij leggen de verschillende onderdelen berichtjes voor elkaar klaar, die dan zo snel mogelijk opgepikt dienen te worden. Deze stijl van integratie is één van de grotere technische uitdagingen van deze cursus en zal uitgebreid worden behandeld in <<_messaging>> en <<_messaging_events>>.

=== Communicatiewijzes

Naast een algemene stijl van integratie zijn er ook verschillende manieren waarop de verschillende nodes in je systeem met elkaar communiceren. In cite:[hohpe_enterprise_2012] onderscheiden we grofweg drie manieren:

* Fire & Forget
* Request-Reply
* Publish-Subscribe

Degene die verreweg het meest bekend zal zijn is Request-Reply. Tot nu toe waren onze backends voornamelijk HTTP gericht, dus met een HTTP-Request en een HTTP-Response (de reply). Hierin verwachten we altijd een antwoord op ons verzoek, al is het maar puur een erkenning dat het Request correct begrepen en/of uitgevoerd is. Uiteraard is het nadeel van een Request-Reply strategie dat je alle Fallacies of Distributed systems dubbel tegenkomt! Op de heenweg (het Request) en op de terugweg (de Reply). Meestal gebruikt een RPC integratie Request-Reply als communicatiewijze.

Als je per se tegen de stroom in wilt, dan zou je kunnen beweren dat een Request/Acknowledge HTTP interactie een voorbeeld is van Fire & Forget met RPC. Bij Request/Acknowledge checkt de ontvangende partij puur of het ingekomen bericht begrijpelijk is, en stuurt zo snel mogelijk een HTTP-status 202-Accepted terug. Het daadwerkelijke werk gebeurt dan op bijv. een andere Thread. Een voorbeeld hiervan zijn bijv. de algemene AVG & Privacy-rapporten die je kan opvragen bij Social Media providers. Dan verzoek je al je data en krijg je een paar dagen later een berichtje dat je een zipje/pdfje/ander-verslagje kan downloade.

Publish-Subscribe kan natuurlijk ook over RPC, dan denk je bijv. aan zogeheten Web-Hooks. Zo kun je bij Github bijv. voor verschillende _Events_ (zoals een push, of een gefaalde CI-build, of iets dergelijks) één of meerdere URLs opgeven waar Github dan een bepaalde POST naartoe stuurt als die gebeurtenis optreedt.

In het geval van Fire & Forget sturen we een bericht naar een andere service. En dat was het. We weten niet _OF_ het is aangekomen, _OF_ het begrepen is, _OF_ dat het misschien grandioos is misgegaan. Weten we allemaal niet. En, als dit acceptabel is in onze usecase, dan is dat ook heel bevrijdend! We weten het niet, en hoeven er dus ook geen rekening mee te houden in ons systeem! (sterker nog, dat kunnen we helemaal niet). Een voorbeeld is als je bijv. informatie wilt loggen en dat naar een ander systeem wilt brengen. Meestal is die informatie van vrij lage waarde, dus we willen absoluut niet het risico lopen dat een fout in het versturen van onze logging-informatie de echt belangrijke business-processen tot een halt roept. In de praktijk gebruiken we een Fire & Forget interactie ook wel in het wat grijzere gebied waarbij we een bericht versturen, en de informatie over hoe het is gegaan op een totaal andere wijze, vaak veel later, pas terug komt. Neem bijv. het versturen van een e-mail. Als dit niet lukt (bijv. omdat het andere emailadres niet bestaat, of omdat er iets qua spam-regels niet helemaal soepel gaat), dan hoor je dit vaak pas uren/dagen later, via een totaal ander kanaal. Het is dan een beetje een grijs gebied of dit Request-Reply, of twee losse Fire & Forgets zijn. 

Tot slot is er Publish-Subscribe, dat is het standaard Observer-Pattern, maar dan voor communicatie tussen processen. Bij het Observer-pattern is er een Event (bijv. een Button-Click), waar meerdere functies zich aan kunnen hangen (_subscriben_ als een _observer_). Op het moment dat er daadwerkelijk geclicked wordt, dan wordt de click-event gepublished naar alle subscribers; oftewel al die functies worden uitgevoerd.
Bij de Publish-Subscribe integratie-wijze meldt je een service dus aan om bepaalde berichten te ontvangen. Dit zorgt er voor dat de service waar informatie vandaan komt niet hoeft te weten _OF_, en zo ja, _hoeveel_ services er van diens informatie afhankelijk zijn. Dat is ook heel bevrijdend qua werklast. Jij kan als service-developer gewoon je ding doen, services komen wel naar jou toe als ze interesse hebben. Zo'n event wordt vaak via eenzelfde systeem als Fire & Forget gepublished, dus één kant op, met geen fancy foutafhandeling. Dat is ook belangrijk, want je wil niet dat één subscriber er voor kan zorgen dat alle andere subscribers hun berichten niet meer ontvangen!

Eenvoudige messaging integraties maken vaak gebruik van Fire & Forget, en zodra er meer & meer services ontstaan evolueert dit vaak naar Publish/Subscribe. Het is uiteraard ook mogelijk om Request/Reply interacties op te zetten met Messaging, maar dat is iets minder gebruikelijk (het is gewoon net iets lastiger qua frameworks & code, dan om in dat soort gevallen een RPC call te doen). Spring-AMQP heeft bijv. een aantal SendAndWaitForReply methodes, maar dat is vaak een beetje zonde, omdat jouw applicatie dan middenin een stukje server-code gaat zitten wachten op een reply. Dat is voor een webserver vaak vragen om performance-problemen.

[#coupling]
=== Coupling

Bij Software Architecture hebben we het gehad over Coupling: hoeveel afhankelijkheid hebben stukken code van elkaar? De koppeling die we daar hebben besproken was primair gericht op compile-time dependencies. Welke andere classes of modules gebruikt een class of module om z'n werk te doen. Dit concept van koppeling kun je breder trekken, naar verschillende soorten koppeling. In deze cursus spelen 3 andere soorten Coupling ook een grote rol:

* Spatial Coupling (ook wel Referential Coupling genoemd): services weten van elkaar in detail *waar* ze zijn
* Temporal Coupling: services zijn sterk afhankelijk van elkaars temporele eigenschappen, ze moeten bijv. tegelijk online zijn, of als de één traag is wordt de ander dat ook.
* Semantic Coupling: De ene service weet stiekem heel veel details van hoe de ander z'n werkt doet, en maakt daar ook gebruikt van.

Als voorbeeld nemen we een bestellings-applicatie (van zeg... broodroosters), met daarachter een magazijn-applicatie (die bijhoudt hoeveel we er per soort nog op voorraad hebben) en een bezorgings-applicatie die de bestelde broodroosters tot aan je deur brengt. En laten we een aantal mogelijke manieren doorlopen waarop dit systeem geïmplementeerd zou kunnen zijn...

Stel de bestellings-applicatie stuurt een bericht naar de magazijn-applicatie via HTTP naar een bepaald IP adres (bijv. POST http://192.168.178.42:8082/toasters/reservations), en die magazijnservice stuurt een bezorgings-commando door naar de bezorging (bijv. POST http://192.168.178.99:8080/toasters/deliveries). Deze services weten nu heel precies van elkaar waar de processen draaien (op IP-adres en poort), en omdat het HTTP is moeten ze ook allemaal tegelijk online zijn om deze usecase te laten werken. Stel nu dat de bestelling geannuleerd wordt, en dat in dat geval de bestellings-applicatie _weet_ dat er dan een annulering doorgestuurd moet worden naar *EN* het magazijn, *EN* de bezorging. Dan weet de bestelservice ook nog 's stiekem dat de magazijnservice überhaupt met de bestelservice praat. Deze drie applicaties zijn dus Spatieel, Temporeel en Semantisch sterk geCoupled. De kleinste fout of miscommunicatie tijdens het onderhouden of doorontwikkelen van deze services kan het hele bestelproces om zeep helpen, en dat zal het bedrijf niet leuk vinden.

Als je het zo expliciet leest dan denk je misschien bij jezelf "Dat soort systemen zou ik nooit bouwen!", maar vaak ontstaan dit soort systemen omdat dit soort features langzaam groeien, en een directe coupling bij elke kleine stap met afstand de makkelijkste optie was.

Een alternatieve strategie is om de bestellings-applicatie via Messaging en Publish Subscribe één bericht ("Er is een bestelling voor broodrooster XYZ") te laten afleveren op een gedeelde plek. Zowel de magazijn-applicatie, als de bezorgings-applicatie subscriben aan deze berichten en krijgen dus een kopietje. De annulering kan dan soortgelijk verlopen. In dit scenario weten de applicatie van elkaar niet waar ze gehost zijn, of ze überhaupt online zijn, en al helemaal niet wat er onderling gebeurd. Deze applicaties zijn dus op al deze dimensies ontkoppelt.

Uiteraard is de compleet ontkoppelde wereld ook niet altijd ideaal, want misschien stuurt in dit geval de magazijnservice wel nog steeds een bericht dat de bezorging geannuleerd moet worden, en krijgt de arme bezorger dubbele spam op z'n telefoon. Een super-los-gecoupled systeem kan erg rommelig en chaotisch overkomen. Dus een zekere mate van Coupling geeft ook duidelijkheid en structuur.

Tot slot nog een korte noot over de reeds bekende vorm van Coupling, die van dependencies tussen classes en modules, maar dan in de wereld van gedistribueerde systemen. Een vaak genoemd voordeel van gedistribueerde systemen is dat je verschillende programmeertalen kan mengen (zodat je teams met verschillende expertises kan laten samenwerken). Dan is het uiteraard niet goed mogelijk om ze classes en modules te laten delen (je kunt immers in C# geen Java classes inladen). 

Ook als dit wel mogelijk is (omdat alle applicaties in dezelfde taal zijn geschreven) is het heel erg oppassen geblazen. Het algemene advies is om zowieso geen afhankelijkheid in te bouwen tussen services (dus Service A zou alle classes van Service B mogen gebruiken), dat levert teveel chaosfootnote:[Spring doet hier technisch ook heel moeilijk over. De Spring-boot-maven plugin herschrijft je JAR files zodanig dat je ze niet als dependency meer kan gebruiken. Geluk bij een ongeluk, wat mij betreft.]. 
Op zo'n moment kun je expliciet een extra module starten met daarin alleen gedeelde code, dan is ook voor iedereen duidelijk dat die code niet zomaar aangepast kan worden zonder overleg (of op z'n minst rekening-houden-met). Uiteraard zijn er puristen die zeggen dat je dit nooit mag doen, maar wees hier vooral pragmatisch in. Praktische voorbeelden hiervan zie je in <<multipom>>.

